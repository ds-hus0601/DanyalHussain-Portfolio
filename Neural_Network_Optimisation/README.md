# Neural Network Optimisation and Regularisation

### Objective
Investigate model tuning techniques to enhance performance and reduce overfitting in neural network models trained on structured datasets.

### Approach
- Built baseline neural networks and compared optimisation strategies using Adam and RMSProp.
- Added dropout layers and L2 regularisation to prevent overfitting.
- Conducted multiple experiments with varying batch sizes, epochs, and learning rates to analyse convergence behaviour.

### Tools & Techniques
Python (TensorFlow, Keras, NumPy, Matplotlib, Scikit-learn)

### Results
- Demonstrated improved validation accuracy and reduced overfitting through dropout and weight regularisation.
- RMSProp showed faster convergence on smaller datasets, while Adam delivered stable performance on larger ones.
- Documented comparative metrics for each configuration and visualised trainingâ€“validation loss curves.

### Key Learning
Reinforced the importance of systematic experimentation and regularisation in neural network optimisation, particularly when balancing accuracy with generalisation.

